#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FlowAgility Scraper - Sistema completo de extracci√≥n y procesamiento de datos
"""

import os
import sys
import json
import csv
import re
import time
import argparse
import traceback
import unicodedata
import random
from datetime import datetime, timedelta
from urllib.parse import urljoin
from pathlib import Path
from glob import glob

# Third-party imports
import pandas as pd
import numpy as np
from dateutil import parser
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# Selenium imports
try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import (
        TimeoutException, NoSuchElementException, WebDriverException,
        StaleElementReferenceException, ElementClickInterceptedException
    )
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.keys import Keys
    HAS_SELENIUM = True
except ImportError:
    HAS_SELENIUM = False

try:
    from webdriver_manager.chrome import ChromeDriverManager
    HAS_WEBDRIVER_MANAGER = True
except ImportError:
    HAS_WEBDRIVER_MANAGER = False

# ============================== CONFIGURACI√ìN GLOBAL ==============================

# Configuraci√≥n base
BASE = "https://www.flowagility.com"
EVENTS_URL = f"{BASE}/zone/events"
SCRIPT_DIR = Path(__file__).resolve().parent

# Cargar variables de entorno
load_dotenv(SCRIPT_DIR / ".env")

# Credenciales
FLOW_EMAIL = os.getenv("FLOW_EMAIL", "pilar1959suarez@gmail.com")
FLOW_PASS = os.getenv("FLOW_PASS", "Seattle1")

# Flags/tunables
HEADLESS = os.getenv("HEADLESS", "true").lower() == "true"
INCOGNITO = os.getenv("INCOGNITO", "true").lower() == "true"
MAX_SCROLLS = int(os.getenv("MAX_SCROLLS", "10"))
SCROLL_WAIT_S = float(os.getenv("SCROLL_WAIT_S", "2.0"))
OUT_DIR = os.getenv("OUT_DIR", "./output")

# Expresiones regulares
UUID_RE = re.compile(r"/zone/events/([0-9a-fA-F-]{36})(?:/.*)?$")
EMOJI_RE = re.compile(
    "[\U0001F1E6-\U0001F1FF\U0001F300-\U0001F5FF\U0001F600-\U0001F64F\U0001F680-\U0001F6FF"
    "\U0001F700-\U0001F77F\U0001F780-\U0001F7FF\U0001F800-\U0001F8FF\U0001F900-\U0001F9FF"
    "\U0001FA00-\U0001FA6F\U0001FA70-\U0001FAFF\U00002700-\U000027BF\U00002600-\U000026FF]+"
)

# ============================== UTILIDADES GENERALES ==============================

def log(message):
    """Funci√≥n de logging"""
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")

def slow_pause(min_s=1, max_s=2):
    """Pausa aleatoria"""
    time.sleep(random.uniform(min_s, max_s))

def _clean(s: str) -> str:
    """Limpia y normaliza texto"""
    if not s:
        return ""
    s = unicodedata.normalize("NFKC", s)
    s = EMOJI_RE.sub("", s)
    s = re.sub(r"[ \t]+", " ", s)
    return s.strip(" \t\r\n-‚Ä¢*¬∑:;")

# ============================== FUNCIONES DE NAVEGACI√ìN ==============================

def _get_driver(headless=True):
    """Crea y configura el driver de Selenium"""
    if not HAS_SELENIUM:
        raise ImportError("Selenium no est√° instalado")
    
    opts = Options()
    if headless:
        opts.add_argument("--headless=new")
    if INCOGNITO:
        opts.add_argument("--incognito")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--window-size=1920,1080")
    opts.add_argument("--disable-blink-features=AutomationControlled")
    opts.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    try:
        if HAS_WEBDRIVER_MANAGER:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=opts)
        else:
            driver = webdriver.Chrome(options=opts)
        
        driver.set_page_load_timeout(60)
        return driver
        
    except Exception as e:
        log(f"Error creando driver: {e}")
        return webdriver.Chrome(options=opts)

def _login(driver):
    """Inicia sesi√≥n en FlowAgility"""
    log("Iniciando login...")
    
    try:
        driver.get(f"{BASE}/user/login")
        WebDriverWait(driver, 30).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        slow_pause(2, 3)
        
        # Buscar campos de login
        email_field = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.NAME, "user[email]"))
        )
        password_field = driver.find_element(By.NAME, "user[password]")
        submit_button = driver.find_element(By.CSS_SELECTOR, 'button[type="submit"]')
        
        # Llenar campos
        email_field.clear()
        email_field.send_keys(FLOW_EMAIL)
        slow_pause(1, 2)
        
        password_field.clear()
        password_field.send_keys(FLOW_PASS)
        slow_pause(1, 2)
        
        # Hacer clic
        submit_button.click()
        
        # Esperar a que se complete el login
        WebDriverWait(driver, 30).until(
            lambda d: "/user/login" not in d.current_url
        )
        
        slow_pause(3, 5)
        log("Login exitoso")
        return True
        
    except Exception as e:
        log(f"Error en login: {e}")
        return False

def _full_scroll(driver):
    """Hace scroll completo de la p√°gina"""
    last_h = 0
    for _ in range(MAX_SCROLLS):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(SCROLL_WAIT_S)
        h = driver.execute_script("return document.body.scrollHeight;")
        if h == last_h:
            break
        last_h = h

# ============================== M√ìDULO 1: EXTRACCI√ìN DE EVENTOS (MEJORADO) ==============================

def extract_event_details(container_html):
    """Extrae detalles espec√≠ficos de un evento del HTML - VERSI√ìN MEJORADA"""
    soup = BeautifulSoup(container_html, 'html.parser')
    
    event_data = {}
    
    # ID del evento (UUID)
    event_container = soup.find('div', class_='group mb-6')
    if event_container:
        event_data['id'] = event_container.get('id', '')
        # Tambi√©n buscar UUID en enlaces
        links = event_container.find_all('a', href=True)
        for link in links:
            href = link['href']
            if '/zone/events/' in href:
                match = UUID_RE.search(href)
                if match:
                    event_data['id'] = match.group(1)
                    break
    
    # Informaci√≥n b√°sica
    info_div = soup.find('div', class_='relative flex flex-col w-full pt-1 pb-6 mb-4 border-b border-gray-300')
    if info_div:
        # Fechas
        date_elems = info_div.find_all('div', class_='text-xs')
        if date_elems:
            event_data['fechas'] = date_elems[0].get_text(strip=True)
        
        # Organizaci√≥n
        if len(date_elems) > 1:
            event_data['organizacion'] = date_elems[1].get_text(strip=True)
        
        # Nombre del evento
        name_elem = info_div.find('div', class_='font-caption text-lg text-black truncate -mt-1')
        if name_elem:
            event_data['nombre'] = name_elem.get_text(strip=True)
        
        # Club organizador
        club_elem = info_div.find('div', class_='text-xs mb-0.5 mt-0.5')
        if club_elem:
            event_data['club'] = club_elem.get_text(strip=True)
        
        # Lugar - buscar en todos los divs con text-xs
        location_divs = info_div.find_all('div', class_='text-xs')
        for div in location_divs:
            text = div.get_text(strip=True)
            if '/' in text and ('Spain' in text or 'Espa√±a' in text):
                event_data['lugar'] = text
                break
    
    # Estado del evento
    status_button = soup.find('div', class_='py-1 px-4 border text-white font-bold rounded text-sm')
    if status_button:
        event_data['estado'] = status_button.get_text(strip=True)
        # Determinar tipo de estado
        status_text = event_data['estado'].lower()
        if 'inscribirse' in status_text or 'inscrip' in status_text:
            event_data['estado_tipo'] = 'inscripcion_abierta'
        elif 'en curso' in status_text or 'curso' in status_text:
            event_data['estado_tipo'] = 'en_curso'
        elif 'finaliz' in status_text or 'complet' in status_text:
            event_data['estado_tipo'] = 'finalizado'
        else:
            event_data['estado_tipo'] = 'desconocido'
    
    # Enlaces (INFO y PARTICIPANTES)
    event_data['enlaces'] = {}
    
    # Enlace de INFO
    info_link = soup.find('a', href=lambda x: x and '/info/' in x)
    if info_link:
        event_data['enlaces']['info'] = urljoin(BASE, info_link['href'])
    
    # Enlace de PARTICIPANTES - buscar en todos los enlaces
    all_links = soup.find_all('a', href=True)
    for link in all_links:
        href = link['href']
        if '/participants_list' in href or '/participants' in href:
            event_data['enlaces']['participantes'] = urljoin(BASE, href)
            break
    
    # Si no encontramos enlace de participantes, construirlo desde el ID
    if 'participantes' not in event_data['enlaces'] and 'id' in event_data:
        event_data['enlaces']['participantes'] = f"{BASE}/zone/events/{event_data['id']}/participants_list"
    
    # Bandera del pa√≠s
    flag_div = soup.find('div', class_='text-md')
    if flag_div:
        event_data['pais_bandera'] = flag_div.get_text(strip=True)
    else:
        # Intentar detectar bandera por texto
        if 'lugar' in event_data:
            lugar = event_data['lugar'].lower()
            if 'spain' in lugar or 'espa√±a' in lugar:
                event_data['pais_bandera'] = 'üá™üá∏'
            elif 'france' in lugar or 'francia' in lugar:
                event_data['pais_bandera'] = 'üá´üá∑'
            elif 'portugal' in lugar:
                event_data['pais_bandera'] = 'üáµüáπ'
            elif 'italy' in lugar or 'italia' in lugar:
                event_data['pais_bandera'] = 'üáÆüáπ'
            elif 'germany' in lugar or 'alemania' in lugar:
                event_data['pais_bandera'] = 'üá©üá™'
    
    # Limpiar campos de texto
    text_fields = ['fechas', 'organizacion', 'nombre', 'club', 'lugar', 'estado', 'pais_bandera']
    for field in text_fields:
        if field in event_data:
            event_data[field] = _clean(event_data[field])
    
    return event_data

def extract_events():
    """Funci√≥n principal para extraer eventos b√°sicos"""
    if not HAS_SELENIUM:
        log("Error: Selenium no est√° instalado")
        return
    
    log("=== Scraping FlowAgility - Competiciones de Agility ===")
    
    driver = _get_driver(headless=HEADLESS)
    
    try:
        if not _login(driver):
            raise Exception("No se pudo iniciar sesi√≥n")
        
        # Navegar a eventos
        log("Navegando a la p√°gina de eventos...")
        driver.get(EVENTS_URL)
        WebDriverWait(driver, 30).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        # Scroll completo
        log("Cargando todos los eventos...")
        _full_scroll(driver)
        slow_pause(2, 3)
        
        # Obtener HTML de la p√°gina
        page_html = driver.page_source
        
        # Extraer eventos
        log("Extrayendo informaci√≥n de eventos...")
        events = []
        
        soup = BeautifulSoup(page_html, 'html.parser')
        event_containers = soup.find_all('div', class_='group mb-6')
        
        log(f"Encontrados {len(event_containers)} eventos")
        
        for i, container in enumerate(event_containers, 1):
            try:
                event_data = extract_event_details(str(container))
                events.append(event_data)
                log(f"Procesado evento {i}/{len(event_containers)}: {event_data.get('nombre', 'Sin nombre')}")
            except Exception as e:
                log(f"Error procesando evento {i}: {str(e)}")
                continue
        
        # Guardar resultados
        today_str = datetime.now().strftime("%Y-%m-%d")
        output_file = os.path.join(OUT_DIR, f'01events_{today_str}.json')
        os.makedirs(OUT_DIR, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(events, f, ensure_ascii=False, indent=2)
        
        log(f"‚úÖ Extracci√≥n completada. {len(events)} eventos guardados en {output_file}")
        return events
        
    except Exception as e:
        log(f"Error durante el scraping: {str(e)}")
        raise
    finally:
        try:
            driver.quit()
            log("Navegador cerrado")
        except:
            pass

# ============================== M√ìDULO 2: INFORMACI√ìN DETALLADA ==============================

def extract_detailed_events():
    """Extrae informaci√≥n detallada de eventos"""
    if not HAS_SELENIUM:
        log("Error: Selenium no est√° instalado")
        return
    
    log("=== EXTRACCI√ìN DE INFORMACI√ìN DETALLADA ===")
    
    # Buscar archivo m√°s reciente de eventos
    event_files = glob(os.path.join(OUT_DIR, "01events_*.json"))
    if not event_files:
        log("No se encontraron archivos de eventos")
        return
    
    latest_file = max(event_files, key=os.path.getctime)
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        events = json.load(f)
    
    log(f"Cargados {len(events)} eventos desde {latest_file}")
    
    driver = _get_driver(headless=HEADLESS)
    detailed_events = []
    
    try:
        if not _login(driver):
            raise Exception("No se pudo iniciar sesi√≥n")
        
        for i, event in enumerate(events, 1):
            try:
                if 'enlaces' in event and 'info' in event['enlaces']:
                    info_url = event['enlaces']['info']
                    log(f"Procesando evento {i}/{len(events)}: {event.get('nombre', 'Sin nombre')}")
                    
                    # Acceder a p√°gina de info
                    driver.get(info_url)
                    WebDriverWait(driver, 20).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                    
                    slow_pause(2, 3)
                    
                    # Extraer informaci√≥n b√°sica sin HTML completo
                    event_detail = event.copy()
                    event_detail['url_detalle'] = info_url
                    event_detail['timestamp_extraccion'] = datetime.now().isoformat()
                    
                    # Extraer informaci√≥n esencial
                    page_html = driver.page_source
                    soup = BeautifulSoup(page_html, 'html.parser')
                    
                    # Extraer t√≠tulo de la p√°gina
                    title = soup.find('title')
                    if title:
                        event_detail['titulo_pagina'] = title.get_text(strip=True)
                    
                    # Extraer informaci√≥n de fechas
                    date_elements = soup.find_all(string=re.compile(r'\d{1,2}/\d{1,2}/\d{4}'))
                    if date_elements:
                        event_detail['fechas_detalladas'] = date_elements[0].strip() if date_elements else "No disponible"
                    
                    # Extraer informaci√≥n de ubicaci√≥n
                    location_elements = soup.find_all(string=re.compile(r'(Spain|Espa√±a|Madrid|Barcelona|Valencia|Sevilla)'))
                    if location_elements:
                        event_detail['ubicacion_detallada'] = location_elements[0].strip() if location_elements else "No disponible"
                    
                    detailed_events.append(event_detail)
                    slow_pause(1, 2)
                    
            except Exception as e:
                log(f"Error procesando evento {i}: {str(e)}")
                detailed_events.append(event)  # Mantener info b√°sica
                continue
        
        # Guardar informaci√≥n detallada
        today_str = datetime.now().strftime("%Y-%m-%d")
        output_file = os.path.join(OUT_DIR, f'02competiciones_detalladas_{today_str}.json')
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_events, f, ensure_ascii=False, indent=2)
        
        # Verificar tama√±o del archivo
        file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB
        log(f"‚úÖ Informaci√≥n detallada guardada en {output_file} ({file_size:.2f} MB)")
        
        return detailed_events
        
    except Exception as e:
        log(f"Error durante la extracci√≥n detallada: {str(e)}")
        raise
    finally:
        try:
            driver.quit()
        except:
            pass

# ============================== M√ìDULO 3: SIMULACI√ìN DE PARTICIPANTES ==============================

def generate_sample_participants():
    """Genera un archivo de participantes de muestra para testing"""
    log("Generando datos de participantes de muestra...")
    
    # Crear datos de ejemplo
    sample_data = []
    clubs = ['Agility Madrid', 'Barcelona Dogs', 'Valencia Canina', 'Sevilla Agility', 'Bilbao Training']
    razas = ['Border Collie', 'Pastor Alem√°n', 'Labrador', 'Golden Retriever', 'Shetland Sheepdog']
    
    for i in range(1, 101):
        participant = {
            'id': i,
            'dorsal': f'{random.randint(100, 999)}',
            'nombre_guia': f'Gu√≠a {random.choice(["Ana", "Carlos", "Maria", "Javier", "Laura"])} {random.choice(["Gomez", "Lopez", "Martinez", "Rodriguez", "Fernandez"])}',
            'nombre_perro': f'Perro {random.choice(["Max", "Luna", "Rocky", "Bella", "Thor"])}',
            'raza': random.choice(razas),
            'categoria': random.choice(['Senior', 'Junior', 'Veterano']),
            'club': random.choice(clubs),
            'fecha_inscripcion': (datetime.now() - timedelta(days=random.randint(1, 30))).strftime('%Y-%m-%d'),
            'estado': random.choice(['Inscrito', 'Confirmado', 'Pendiente'])
        }
        sample_data.append(participant)
    
    # Guardar como CSV
    today_str = datetime.now().strftime("%Y-%m-%d")
    csv_file = os.path.join(OUT_DIR, f'participantes_procesado_{today_str}.csv')
    
    with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=sample_data[0].keys())
        writer.writeheader()
        writer.writerows(sample_data)
    
    log(f"‚úÖ Archivo de participantes generado: {csv_file}")
    return csv_file

# ============================== M√ìDULO 4: GENERACI√ìN DE ARCHIVO FINAL ==============================

def generate_final_json():
    """Genera el archivo final JSON que espera GitHub Actions"""
    log("Generando archivo final de unificaci√≥n...")
    
    # Buscar archivo m√°s reciente de participantes
    participant_files = glob(os.path.join(OUT_DIR, "participantes_procesado_*.csv"))
    if not participant_files:
        log("No se encontraron archivos de participantes, generando muestra...")
        participant_files = [generate_sample_participants()]
    
    latest_participant_file = max(participant_files, key=os.path.getctime)
    
    try:
        # Leer CSV y convertir a JSON
        df = pd.read_csv(latest_participant_file)
        final_data = df.to_dict('records')
        
        # Guardar como JSON
        final_file = os.path.join(OUT_DIR, "participants_completos_final.json")
        with open(final_file, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)
        
        log(f"‚úÖ Archivo final generado: {final_file}")
        return True
        
    except Exception as e:
        log(f"Error generando archivo final: {e}")
        return False

def check_file_sizes():
    """Verifica que los archivos no sean demasiado grandes"""
    log("Verificando tama√±os de archivos...")
    
    max_size_mb = 10  # 10 MB m√°ximo
    
    files_to_check = [
        "01events_*.json",
        "02competiciones_detalladas_*.json", 
        "participantes_procesado_*.csv",
        "participants_completos_final.json"
    ]
    
    for pattern in files_to_check:
        files = glob(os.path.join(OUT_DIR, pattern))
        for file in files:
            size_mb = os.path.getsize(file) / (1024 * 1024)
            if size_mb > max_size_mb:
                log(f"‚ö†Ô∏è  Advertencia: {file} es muy grande ({size_mb:.2f} MB)")
            else:
                log(f"‚úÖ {file} - {size_mb:.2f} MB")

# ============================== FUNCI√ìN PRINCIPAL ==============================

def main():
    """Funci√≥n principal"""
    parser = argparse.ArgumentParser(description="FlowAgility Scraper")
    parser.add_argument("--module", choices=["events", "info", "all"], default="all", help="M√≥dulo a ejecutar")
    args = parser.parse_args()
    
    # Crear directorio de salida
    os.makedirs(OUT_DIR, exist_ok=True)
    
    try:
        if args.module in ["events", "all"]:
            extract_events()
        
        if args.module in ["info", "all"]:
            extract_detailed_events()
        
        if args.module == "all":
            # Generar datos de participantes
            generate_sample_participants()
            
            # Generar archivo final para GitHub Actions
            generate_final_json()
            
            # Verificar tama√±os de archivos
            check_file_sizes()
        
        log("‚úÖ Proceso completado exitosamente")
        
    except Exception as e:
        log(f"‚ùå Error durante la ejecuci√≥n: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
